{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import xarray as xr \n",
    "import torch\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_2011.nc\")\n",
    "co_no2_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_CO_NO2.nc\")\n",
    "no_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_NO.nc\")\n",
    "humidity_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_SpecificHum.nc\")\n",
    "temp_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_2011_Archi_T.nc\")\n",
    "TIME_POINTS, PRESSURE_POINTS, LAT_POINTS, LON_POINTS = ozone_2011[\"o3\"].shape\n",
    "\n",
    "combined_data = xr.Dataset(\n",
    "    {\n",
    "        \"ozone\": ozone_2011[\"o3\"],\n",
    "        \"pm25\": ozone_2011[\"PM2_5_DRY\"],\n",
    "        \"co\":  co_no2_2011[\"co\"],\n",
    "        \"no2\": co_no2_2011[\"no2\"],\n",
    "        \"no\": no_2011[\"no\"],\n",
    "        \"humidity\": humidity_2011[\"QVAPOR\"],\n",
    "        \"temperature\": temp_2011[\"T2\"].expand_dims({\"bottom_top\": np.arange(PRESSURE_POINTS)}, axis=1)\n",
    "    },\n",
    "    coords={\"time\": ozone_2011[\"Times\"], \n",
    "            \"pressure\": ozone_2011[\"bottom_top\"], \n",
    "            \"latitude\": ozone_2011[\"south_north\"], \n",
    "            \"longitude\": ozone_2011[\"west_east\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, use_activation: bool, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias = True)\n",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True) if use_activation else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.conv(x))\n",
    "    \n",
    "class DenseResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, channels = 32, beta: float = 0.2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.beta = beta \n",
    "        self.conv = nn.ModuleList()\n",
    "\n",
    "        for block_no in range(5):\n",
    "            self.conv.append(ConvBlock(in_channels + channels * block_no, \n",
    "                                       channels if block_no < 4 else in_channels,\n",
    "                                         use_activation=True if block_no < 4 else False,\n",
    "                                           kernel_size=3, stride=1, padding=1))\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        new_inputs = x\n",
    "        for block in self.conv:\n",
    "            out = block(new_inputs)\n",
    "            new_inputs = torch.cat([new_inputs, out], dim=1)\n",
    "        return self.beta * out + x\n",
    "    \n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    def __init__(self, in_channels, residual_beta, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.residual_beta = residual_beta\n",
    "        self.rrdb = nn.Sequential(*[DenseResidualBlock(in_channels, beta=residual_beta) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.rrdb(x) * self.residual_beta + x\n",
    "    \n",
    "\n",
    "class ModifiedSRCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_blocks: int, \n",
    "                 n1: int, n2: int, f1: int, f2: int, f3: int,\n",
    "                 *args, **kwargs) -> None:\n",
    "        \"\"\" Initialize the SRCNN with Dense Residual network model with the required layers \n",
    "         Below params are the hyperparameters for the SRCNN model without the \n",
    "         Bassic block which has been added extra other than the resisual connections.\n",
    "        in_channels (int): Input number of channels\n",
    "        num_blocks (int): Number of RRDB blocks\n",
    "        n1 (int): Number of filters in the first convolutional layer\n",
    "        n2 (int): Number of filters in the second convolutional layer\n",
    "        f1 (int): Kernel size of the first convolutional layer\n",
    "        f2 (int): Kernel size of the second convolutional layer\n",
    "        f3 (int): Kernel size of the third convolutional layer\n",
    "        residual_beta (float): Residual connection weight\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = ConvBlock(in_channels, n1, kernel_size=f1, stride=1, padding=(1, 1, 1), use_activation=True)\n",
    "        self.blocks = nn.Sequential(*[RRDB(n1 + in_channels, residual_beta=0.5) for _ in range(num_blocks)])\n",
    "        self.conv2 = ConvBlock(2 * (n1 + in_channels), n2, kernel_size=f2, stride=1, padding=(1, 1, 1), use_activation=True)\n",
    "        self.conv3 = ConvBlock(n2 + n1 + in_channels, in_channels, kernel_size=f3, stride=1, padding=1, use_activation=False)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        initial = x \n",
    "        print(f\"Initial shape: {initial.shape}\")\n",
    "        x = self.conv1(x)\n",
    "        x = torch.concat([x, initial], dim = 1)\n",
    "        print(f\"Conv1 shape: {x.shape}\")\n",
    "        initial = x\n",
    "        print(f\"Initial shape: {initial.shape}\")\n",
    "        x = self.blocks(x)\n",
    "        x = torch.concat([x, initial], dim = 1)\n",
    "        print(f\"Blocks shape: {x.shape}\")\n",
    "        x = self.conv2(x)\n",
    "        print(f\"shape after conv 2: {x.shape}\")\n",
    "        x = torch.concat([x, initial], dim=1)\n",
    "        print(f\"concatenated shape : {x.shape}\")\n",
    "        x = self.conv3(x)\n",
    "        print(f\"Final shape = {x.shape}\")\n",
    "        return x \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.rand(32, 7, 50, 20, 20, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "in_channels = 7\n",
    "num_blocks = 3\n",
    "n1 = 32\n",
    "n2 = 64\n",
    "f1 = 3\n",
    "f2 = 3\n",
    "f3 = 3\n",
    "srcnn = ModifiedSRCNN(in_channels = in_channels, num_blocks = num_blocks, n1 = n1, n2 = n2, f1 = f1, f2 = f2, f3 = f3).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdilation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies a 3D convolution over an input signal composed of several input\n",
      "planes.\n",
      "\n",
      "In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`\n",
      "and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:\n",
      "\n",
      ".. math::\n",
      "    out(N_i, C_{out_j}) = bias(C_{out_j}) +\n",
      "                            \\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star input(N_i, k)\n",
      "\n",
      "where :math:`\\star` is the valid 3D `cross-correlation`_ operator\n",
      "\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "* :attr:`stride` controls the stride for the cross-correlation.\n",
      "\n",
      "* :attr:`padding` controls the amount of padding applied to the input. It\n",
      "  can be either a string {'valid', 'same'} or a tuple of ints giving the\n",
      "  amount of implicit padding applied on both sides.\n",
      "\n",
      "* :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.\n",
      "  It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.\n",
      "\n",
      "* :attr:`groups` controls the connections between inputs and outputs.\n",
      "  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
      "  :attr:`groups`. For example,\n",
      "\n",
      "    * At groups=1, all inputs are convolved to all outputs.\n",
      "    * At groups=2, the operation becomes equivalent to having two conv\n",
      "      layers side by side, each seeing half the input channels\n",
      "      and producing half the output channels, and both subsequently\n",
      "      concatenated.\n",
      "    * At groups= :attr:`in_channels`, each input channel is convolved with\n",
      "      its own set of filters (of size\n",
      "      :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\n",
      "\n",
      "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
      "\n",
      "    - a single ``int`` -- in which case the same value is used for the depth, height and width dimension\n",
      "    - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,\n",
      "      the second `int` for the height dimension and the third `int` for the width dimension\n",
      "\n",
      "Note:\n",
      "    When `groups == in_channels` and `out_channels == K * in_channels`,\n",
      "    where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n",
      "\n",
      "    In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n",
      "    a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n",
      "    :math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\n",
      "\n",
      "Note:\n",
      "    In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      "\n",
      "Note:\n",
      "    ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "    the input so the output has the shape as the input. However, this mode\n",
      "    doesn't support any stride values other than 1.\n",
      "\n",
      "Note:\n",
      "    This module supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      "\n",
      "Args:\n",
      "    in_channels (int): Number of channels in the input image\n",
      "    out_channels (int): Number of channels produced by the convolution\n",
      "    kernel_size (int or tuple): Size of the convolving kernel\n",
      "    stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      "    padding (int, tuple or str, optional): Padding added to all six sides of\n",
      "        the input. Default: 0\n",
      "    padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
      "    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
      "    groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
      "    bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
      "\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` or :math:`(C_{in}, D_{in}, H_{in}, W_{in})`\n",
      "    - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` or :math:`(C_{out}, D_{out}, H_{out}, W_{out})`,\n",
      "      where\n",
      "\n",
      "      .. math::\n",
      "          D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
      "                \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
      "\n",
      "      .. math::\n",
      "          H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
      "                \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
      "\n",
      "      .. math::\n",
      "          W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]\n",
      "                \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n",
      "\n",
      "Attributes:\n",
      "    weight (Tensor): the learnable weights of the module of shape\n",
      "                     :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
      "                     :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})`.\n",
      "                     The values of these weights are sampled from\n",
      "                     :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                     :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}`\n",
      "    bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,\n",
      "                     then the values of these weights are\n",
      "                     sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                     :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}`\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # With square kernels and equal stride\n",
      "    >>> m = nn.Conv3d(16, 33, 3, stride=2)\n",
      "    >>> # non-square kernels and unequal stride and with padding\n",
      "    >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n",
      "    >>> input = torch.randn(20, 16, 10, 50, 100)\n",
      "    >>> output = m(input)\n",
      "\n",
      ".. _cross-correlation:\n",
      "    https://en.wikipedia.org/wiki/Cross-correlation\n",
      "\n",
      ".. _link:\n",
      "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LazyConv3d, Conv3d, ConvBn3d, Conv3d"
     ]
    }
   ],
   "source": [
    "nn.Conv3d?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
