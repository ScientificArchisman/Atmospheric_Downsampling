{"cells":[{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T18:12:09.833600Z","iopub.status.busy":"2024-08-02T18:12:09.833213Z","iopub.status.idle":"2024-08-02T18:12:14.062436Z","shell.execute_reply":"2024-08-02T18:12:14.061416Z","shell.execute_reply.started":"2024-08-02T18:12:09.833564Z"},"trusted":true},"outputs":[],"source":["import os \n","import numpy as np \n","import time \n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import xarray as xr\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.data import DataLoader, Dataset, random_split"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loading\n","In this block of code, we define a custom dataser class using pytorch `Dataset` class for our dataset. Following this, we create `Dataloaders` for pairwise data of `high_res_data` and `low_res_data`."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T18:12:14.064967Z","iopub.status.busy":"2024-08-02T18:12:14.064499Z","iopub.status.idle":"2024-08-02T18:12:14.079999Z","shell.execute_reply":"2024-08-02T18:12:14.079083Z","shell.execute_reply.started":"2024-08-02T18:12:14.064934Z"},"trusted":true},"outputs":[],"source":["class MinMaxScaleTransform:\n","    def __init__(self, high_res_data, low_res_data, use_half=False):\n","        self.use_half = use_half\n","\n","        # Compute min and max for each variable at each time point using numpy\n","        self.high_res_mins = np.amin(high_res_data, axis=(2, 3), keepdims=True)\n","        self.high_res_maxs = np.amax(high_res_data, axis=(2, 3), keepdims=True)\n","        self.low_res_mins = np.amin(low_res_data, axis=(2, 3), keepdims=True)\n","        self.low_res_maxs = np.amax(low_res_data, axis=(2, 3), keepdims=True)\n","\n","    def __call__(self, sample):\n","        high_res, low_res = sample\n","        dtype = torch.float16 if self.use_half else torch.float32\n","        high_res = (high_res - self.high_res_mins) / (self.high_res_maxs - self.high_res_mins)\n","        low_res = (low_res - self.low_res_mins) / (self.low_res_maxs - self.low_res_mins)\n","        \n","        return torch.tensor(high_res, dtype=dtype), torch.tensor(low_res, dtype=dtype)\n","\n","\n","class WRFDataset(Dataset):\n","    def __init__(self, high_res_data, low_res_data, chunk_size_lat, chunk_size_long, transform=None):\n","        self.high_res_data = high_res_data\n","        self.low_res_data = low_res_data\n","        self.chunk_size_lat = chunk_size_lat\n","        self.chunk_size_long = chunk_size_long\n","        self.transform = transform\n","        \n","        # Ensure both datasets have the same shape\n","        assert high_res_data.shape == low_res_data.shape, \"High-res and low-res data must have the same shape\"\n","        \n","        # Calculate the number of chunks\n","        self.n_chunks_lat = high_res_data.shape[2] // chunk_size_lat\n","        self.n_chunks_long = high_res_data.shape[3] // chunk_size_long\n","\n","        # Calculate the total number of chunks\n","        self.n_chunks = self.n_chunks_lat * self.n_chunks_long\n","\n","    def __len__(self):\n","        return self.n_chunks\n","\n","    def __getitem__(self, idx):\n","        # Calculate the chunk's starting indices for latitude and longitude\n","        lat_idx = idx // self.n_chunks_long\n","        long_idx = idx % self.n_chunks_long\n","        \n","        lat_start = lat_idx * self.chunk_size_lat\n","        lat_end = lat_start + self.chunk_size_lat\n","        long_start = long_idx * self.chunk_size_long\n","        long_end = long_start + self.chunk_size_long\n","        \n","        high_res_chunk = self.high_res_data[:, :, lat_start:lat_end, long_start:long_end]\n","        low_res_chunk = self.low_res_data[:, :, lat_start:lat_end, long_start:long_end]\n","        \n","        sample = (high_res_chunk, low_res_chunk)\n","        \n","        if self.transform:\n","            sample = self.transform(sample)\n","        \n","        return sample\n","\n","def create_loaders(dataset, batch_size: int = 16):\n","    # Split indices\n","    total_size = len(dataset)\n","    train_size = int(0.8 * total_size)\n","    train_dataset, test_dataset = random_split(dataset, [train_size, total_size - train_size])\n","\n","    valid_size = int(0.2 * len(train_dataset))\n","    train_size = len(train_dataset) - valid_size\n","    train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, valid_loader, test_loader"]},{"cell_type":"markdown","metadata":{},"source":["## Model Definition\n","Here, we define the model. The model is a custom modified model of SRCNN (https://arxiv.org/abs/1501.00092). It has been modified by using residual Connections abd Deep Residual Blocks as shown in the ESRGAN paper (https://arxiv.org/abs/1809.00219)."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T18:12:14.081835Z","iopub.status.busy":"2024-08-02T18:12:14.081455Z","iopub.status.idle":"2024-08-02T18:12:14.106986Z","shell.execute_reply":"2024-08-02T18:12:14.106018Z","shell.execute_reply.started":"2024-08-02T18:12:14.081800Z"},"trusted":true},"outputs":[],"source":["class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, use_activation: bool, *args, **kwargs) -> None:\n","        super().__init__(*args, **kwargs)\n","        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias = True)\n","        self.activation = nn.LeakyReLU(0.2, inplace=True) if use_activation else nn.Identity()\n","\n","    def forward(self, x):\n","        return self.activation(self.conv(x))\n","    \n","class DenseResidualBlock(nn.Module):\n","    def __init__(self, in_channels: int, channels = 32, beta: float = 0.2, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.beta = beta \n","        self.conv = nn.ModuleList()\n","\n","        for block_no in range(5):\n","            self.conv.append(ConvBlock(in_channels + channels * block_no, \n","                                       channels if block_no < 4 else in_channels,\n","                                         use_activation=True if block_no < 4 else False,\n","                                           kernel_size=3, stride=1, padding=1))\n","            \n","            \n","    def forward(self, x):\n","        new_inputs = x\n","        for block in self.conv:\n","            out = block(new_inputs)\n","            new_inputs = torch.cat([new_inputs, out], dim=1)\n","        return self.beta * out + x\n","    \n","\n","class RRDB(nn.Module):\n","    def __init__(self, in_channels, residual_beta, *args, **kwargs) -> None:\n","        super().__init__(*args, **kwargs)\n","        self.residual_beta = residual_beta\n","        self.rrdb = nn.Sequential(*[DenseResidualBlock(in_channels, beta=residual_beta) for _ in range(3)])\n","\n","    def forward(self, x):\n","        return self.rrdb(x) * self.residual_beta + x\n","    \n","\n","class ModifiedSRCNN(nn.Module):\n","    def __init__(self, in_channels: int, num_blocks: int, \n","                 n1: int, n2: int, f1: int, f2: int, f3: int,\n","                 *args, **kwargs) -> None:\n","        \"\"\" Initialize the SRCNN with Dense Residual network model with the required layers \n","         Below params are the hyperparameters for the SRCNN model without the \n","         Bassic block which has been added extra other than the resisual connections.\n","        in_channels (int): Input number of channels\n","        num_blocks (int): Number of RRDB blocks\n","        n1 (int): Number of filters in the first convolutional layer\n","        n2 (int): Number of filters in the second convolutional layer\n","        f1 (int): Kernel size of the first convolutional layer\n","        f2 (int): Kernel size of the second convolutional layer\n","        f3 (int): Kernel size of the third convolutional layer\n","        residual_beta (float): Residual connection weight\n","        \"\"\"\n","        super().__init__(*args, **kwargs)\n","        self.conv1 = ConvBlock(in_channels, n1, kernel_size=f1, stride=1, padding=(1, 1, 1), use_activation=True)\n","        self.bn1 = nn.BatchNorm3d(n1)\n","        self.blocks = nn.Sequential(*[RRDB(n1 + in_channels, residual_beta=0.5) for _ in range(num_blocks)])\n","        self.bn2 = nn.BatchNorm3d((n1 + in_channels))\n","        self.conv2 = ConvBlock(2 * (n1 + in_channels), n2, kernel_size=f2, stride=1, padding=(1, 1, 1), use_activation=True)\n","        self.bn3 = nn.BatchNorm3d(n2)\n","        self.conv3 = ConvBlock(n2 + n1 + in_channels, in_channels, kernel_size=f3, stride=1, padding=1, use_activation=False)\n","        self.bn4 = nn.BatchNorm3d(in_channels)\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        \n","        with torch.cuda.amp.autocast():\n","            initial = x \n","            x = self.conv1(x)\n","            x = self.bn1(x)\n","            x = torch.concat([x, initial], dim = 1)\n","            initial = x\n","            x = self.blocks(x)\n","            x = self.bn2(x)\n","            x = torch.concat([x, initial], dim = 1)\n","            x = self.conv2(x) # Take feature maps here\n","            x = self.bn3(x)\n","            x = torch.concat([x, initial], dim=1)\n","            x = self.conv3(x)\n","            x = self.bn4(x)\n","        return x \n","    \n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                torch.nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    torch.nn.init.zeros_(m.bias)"]},{"cell_type":"markdown","metadata":{},"source":["## Training phase\n","In this block of code, we write the training loop of the model. The model creates a folder called `Logs` during its training. In this folder, (1) it creates a `logs.log` file where it saves the validation losses and the training losses per epoch along with the epoch time and (2) It saves the best weights of the model.\n","An `Early stopping Callback` has been implemented with a patience of `p` for effective training to prevent overfitting of the model."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T18:12:14.109519Z","iopub.status.busy":"2024-08-02T18:12:14.109188Z","iopub.status.idle":"2024-08-02T18:12:14.129939Z","shell.execute_reply":"2024-08-02T18:12:14.128688Z","shell.execute_reply.started":"2024-08-02T18:12:14.109496Z"},"trusted":true},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, \n","                num_epochs, device, log_folder, patience=5):\n","    \"\"\" Train the model using the specified data loaders and hyperparameters.\n","    Saves the best model weights based on the validation loss.\n","    Args:\n","        model (torch.nn.Module): Model to be trained\n","        train_loader (torch.utils.data.DataLoader): Training data loader\n","        val_loader (torch.utils.data.DataLoader): Validation data loader\n","        criterion (torch.nn.Module): Loss function\n","        optimizer (torch.optim.Optimizer): Optimizer\n","        num_epochs (int): Number of epochs to train the model\n","        device (torch.device): Device to run the model on\n","        log_folder (str): Folder to store logs and model weights\n","        patience (int): Number of epochs to wait before early stopping\n","    Returns:\n","        torch.nn.Module: Trained model\n","    \"\"\"\n","    # Move model to the specified device\n","    model.to(device)\n","    \n","    # Create directories for storing artifacts\n","    os.makedirs(log_folder, exist_ok=True)\n","    \n","    log_file = os.path.join(log_folder, 'logs.log')\n","    best_weights_file = os.path.join(log_folder, 'best_weights.pth')\n","    \n","    best_loss = float('inf')\n","    patience_counter = 0\n","    \n","    scaler = GradScaler()\n","    \n","    with open(log_file, 'w') as log:\n","        log.write('Epoch,Train Loss,Val Loss,Epoch Time\\n')\n","        \n","        for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n","            start_time = time.time()\n","            \n","            # Training phase\n","            model.train()\n","            train_losses = []\n","            for hr_images, lr_images in train_loader:\n","                hr_images, lr_images = hr_images.to(device), lr_images.to(device)\n","                optimizer.zero_grad()\n","                \n","                # Enable autocast context for mixed precision training\n","                with autocast():\n","                    sr_images = model(lr_images)\n","                    loss = criterion(sr_images, hr_images)\n","                \n","                # Scale the loss and backward pass\n","                scaler.scale(loss).backward()\n","                \n","                # Clip gradients to prevent exploding gradients\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            \n","                scaler.step(optimizer)\n","                scaler.update()\n","                \n","                train_losses.append(loss.item())\n","            \n","            train_loss = np.mean(train_losses)\n","            \n","            # Validation phase\n","            model.eval()\n","            val_losses = []\n","            with torch.no_grad():\n","                for hr_images, lr_images in val_loader:\n","                    hr_images, lr_images = hr_images.to(device), lr_images.to(device)\n","                    with autocast():\n","                        sr_images = model(lr_images)\n","                        loss = criterion(sr_images, hr_images)\n","                    val_losses.append(loss.item())\n","            \n","            val_loss = np.mean(val_losses)\n","            \n","            epoch_time = time.time() - start_time\n","            \n","            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Epoch Time: {epoch_time:.2f}s')\n","            log.write(f'{epoch+1},{train_loss},{val_loss},{epoch_time}\\n')\n","            \n","            # Check for best validation loss\n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                patience_counter = 0\n","                torch.save(model.state_dict(), best_weights_file)\n","            else:\n","                patience_counter += 1\n","                        \n","            if patience_counter >= patience:\n","                print(f'Early stopping at epoch {epoch+1}')\n","                break\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Configurations \n","Here, we define the model hyperparameters for easy and compact access throughout the training process."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T18:12:14.131633Z","iopub.status.busy":"2024-08-02T18:12:14.131066Z","iopub.status.idle":"2024-08-02T18:12:14.211918Z","shell.execute_reply":"2024-08-02T18:12:14.210286Z","shell.execute_reply.started":"2024-08-02T18:12:14.131592Z"},"trusted":true},"outputs":[],"source":["NUM_EPOCHS: int = 1\n","PATIENCE: int = 15\n","BATCH_SIZE: int= 8\n","LOG_FOLDER: str = \"Logs\"\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","######### Dataloader hyperparameters ########\n","LATITUDE_CHUNK_SIZE = 16\n","LONGITUDE_CHUNK_SIZE = 16\n","\n","######### MODEL HYPERPARAMETERS #########\n","in_channels = 7\n","num_blocks = 2\n","n1 = 32\n","n2 = 128\n","f1 = 3\n","f2 = 3\n","f3 = 3\n","LEARNING_RATE: int = 3e-4"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ozone_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_2011.nc\")\n","co_no2_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_CO_NO2.nc\")\n","no_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_NO.nc\")\n","humidity_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_SpecificHum.nc\")\n","temp_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_2011_Archi_T.nc\")\n","\n","\n","PRESSURE_LEVEL = 2\n","dataset = np.array([ozone_2011[\"o3\"].sel(bottom_top=PRESSURE_LEVEL), \n","            ozone_2011[\"PM2_5_DRY\"].sel(bottom_top=PRESSURE_LEVEL),\n","            co_no2_2011[\"co\"].sel(bottom_top=PRESSURE_LEVEL), \n","            co_no2_2011[\"no2\"].sel(bottom_top=PRESSURE_LEVEL), \n","            no_2011[\"no\"].sel(bottom_top=PRESSURE_LEVEL), \n","            humidity_2011[\"QVAPOR\"].sel(bottom_top=PRESSURE_LEVEL),\n","            temp_2011[\"T2\"]])"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T18:12:14.222524Z","iopub.status.busy":"2024-08-02T18:12:14.219525Z","iopub.status.idle":"2024-08-02T18:12:30.996647Z","shell.execute_reply":"2024-08-02T18:12:30.995849Z","shell.execute_reply.started":"2024-08-02T18:12:14.222478Z"},"trusted":true},"outputs":[],"source":["min_max_transform = MinMaxScaleTransform(dataset, dataset, use_half=True)\n","dataset = WRFDataset(dataset, dataset, LATITUDE_CHUNK_SIZE, LONGITUDE_CHUNK_SIZE, transform=min_max_transform)\n","train_loader, valid_loader, test_loader = create_loaders(dataset, BATCH_SIZE)\n","for i, (high_res_chunk, low_res_chunk) in enumerate(train_loader):\n","    # OUTPUT SHAPE: (BATCH_SIZE, N_VARIABLES, TIME_POINTS, CHUNK_SIZE_LAT, CHUNK_SIZE_LONG)\n","    print(f\"Batch {i}\")\n","    print(f\"High-res chunk shape: {high_res_chunk.shape}\")\n","    print(f\"Low-res chunk shape: {low_res_chunk.shape}\")\n","    break"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T18:12:30.997983Z","iopub.status.busy":"2024-08-02T18:12:30.997703Z","iopub.status.idle":"2024-08-02T18:12:32.671862Z","shell.execute_reply":"2024-08-02T18:12:32.671090Z","shell.execute_reply.started":"2024-08-02T18:12:30.997959Z"},"trusted":true},"outputs":[],"source":["# TRAINING PART\n","model_srcnn = ModifiedSRCNN(in_channels=in_channels, num_blocks=num_blocks, n1=n1, n2=n2, f1=f1, f2=f2, f3=f3)\n","model_srcnn = model_srcnn.half().to(DEVICE)\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model_srcnn.parameters(), lr=LEARNING_RATE)\n","# train_model(model = model_srcnn, train_loader=train_loader, val_loader=valid_loader, \n","#             criterion=criterion, optimizer=optimizer, num_epochs=NUM_EPOCHS, \n","#             log_folder=LOG_FOLDER, device=DEVICE, patience=PATIENCE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5485921,"sourceId":9091067,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
