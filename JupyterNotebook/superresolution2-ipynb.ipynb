{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["import os \n","import numpy as np \n","import time \n","from tqdm import tqdm\n","import xarray as xr\n","import torch\n","from torch import nn\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.utils.data import DataLoader, Dataset, random_split"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loading\n","In this block of code, we define a custom dataser class using pytorch `Dataset` class for our dataset. Following this, we create `Dataloaders` for pairwise data of `high_res_data` and `low_res_data`."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T19:48:38.994044Z","iopub.status.busy":"2024-08-02T19:48:38.993518Z","iopub.status.idle":"2024-08-02T19:48:39.018049Z","shell.execute_reply":"2024-08-02T19:48:39.016800Z","shell.execute_reply.started":"2024-08-02T19:48:38.994006Z"},"trusted":true},"outputs":[],"source":["class MinMaxScaleTransform:\n","    def __init__(self, high_res_data, low_res_data, use_half=False):\n","        self.use_half = use_half\n","\n","        # Compute min and max for each variable at each time point using numpy\n","        self.high_res_mins = np.amin(high_res_data, axis=(2, 3), keepdims=True)\n","        self.high_res_maxs = np.amax(high_res_data, axis=(2, 3), keepdims=True)\n","        self.low_res_mins = np.amin(low_res_data, axis=(2, 3), keepdims=True)\n","        self.low_res_maxs = np.amax(low_res_data, axis=(2, 3), keepdims=True)\n","\n","    def __call__(self, sample):\n","        high_res, low_res = sample\n","        dtype = torch.float16 if self.use_half else torch.float32\n","        high_res = (high_res - self.high_res_mins) / (self.high_res_maxs - self.high_res_mins)\n","        low_res = (low_res - self.low_res_mins) / (self.low_res_maxs - self.low_res_mins)\n","        \n","        return torch.tensor(high_res, dtype=dtype), torch.tensor(low_res, dtype=dtype)\n","\n","\n","class WRFDataset(Dataset):\n","    def __init__(self, high_res_data, low_res_data, chunk_size_lat, chunk_size_long, transform=None):\n","        self.high_res_data = high_res_data\n","        self.low_res_data = low_res_data\n","        self.chunk_size_lat = chunk_size_lat\n","        self.chunk_size_long = chunk_size_long\n","        self.transform = transform\n","        \n","        # Ensure both datasets have the same shape\n","        assert high_res_data.shape == low_res_data.shape, \"High-res and low-res data must have the same shape\"\n","        \n","        # Calculate the number of chunks\n","        self.n_chunks_lat = high_res_data.shape[2] // chunk_size_lat\n","        self.n_chunks_long = high_res_data.shape[3] // chunk_size_long\n","\n","        # Calculate the total number of chunks\n","        self.n_chunks = self.n_chunks_lat * self.n_chunks_long\n","\n","    def __len__(self):\n","        return self.n_chunks\n","\n","    def __getitem__(self, idx):\n","        # Calculate the chunk's starting indices for latitude and longitude\n","        lat_idx = idx // self.n_chunks_long\n","        long_idx = idx % self.n_chunks_long\n","        \n","        lat_start = lat_idx * self.chunk_size_lat\n","        lat_end = lat_start + self.chunk_size_lat\n","        long_start = long_idx * self.chunk_size_long\n","        long_end = long_start + self.chunk_size_long\n","        \n","        high_res_chunk = self.high_res_data[:, :, lat_start:lat_end, long_start:long_end]\n","        low_res_chunk = self.low_res_data[:, :, lat_start:lat_end, long_start:long_end]\n","        \n","        sample = (high_res_chunk, low_res_chunk)\n","        \n","        if self.transform:\n","            sample = self.transform(sample)\n","        \n","        return sample\n","\n","def create_loaders(dataset, batch_size: int = 16):\n","    # Split indices\n","    total_size = len(dataset)\n","    train_size = int(0.8 * total_size)\n","    train_dataset, test_dataset = random_split(dataset, [train_size, total_size - train_size])\n","\n","    valid_size = int(0.2 * len(train_dataset))\n","    train_size = len(train_dataset) - valid_size\n","    train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, valid_loader, test_loader"]},{"cell_type":"markdown","metadata":{},"source":["## Model Definition\n","Here, we define the model. The model is a custom modified model of SRCNN (https://arxiv.org/abs/1501.00092). It has been modified by using residual Connections abd Deep Residual Blocks as shown in the ESRGAN paper (https://arxiv.org/abs/1809.00219)."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T19:48:39.020094Z","iopub.status.busy":"2024-08-02T19:48:39.019720Z","iopub.status.idle":"2024-08-02T19:48:39.051497Z","shell.execute_reply":"2024-08-02T19:48:39.050420Z","shell.execute_reply.started":"2024-08-02T19:48:39.020056Z"},"trusted":true},"outputs":[],"source":["class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, use_activation: bool, *args, **kwargs) -> None:\n","        super().__init__(*args, **kwargs)\n","        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias = True)\n","        self.activation = nn.LeakyReLU(0.2, inplace=True) if use_activation else nn.Identity()\n","\n","    def forward(self, x):\n","        return self.activation(self.conv(x))\n","    \n","class DenseResidualBlock(nn.Module):\n","    def __init__(self, in_channels: int, channels = 32, beta: float = 0.2, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.beta = beta \n","        self.conv = nn.ModuleList()\n","\n","        for block_no in range(5):\n","            self.conv.append(ConvBlock(in_channels + channels * block_no, \n","                                       channels if block_no < 4 else in_channels,\n","                                         use_activation=True if block_no < 4 else False,\n","                                           kernel_size=3, stride=1, padding=1))\n","            \n","            \n","    def forward(self, x):\n","        new_inputs = x\n","        for block in self.conv:\n","            out = block(new_inputs)\n","            new_inputs = torch.cat([new_inputs, out], dim=1)\n","        return self.beta * out + x\n","    \n","\n","class RRDB(nn.Module):\n","    def __init__(self, in_channels, residual_beta, *args, **kwargs) -> None:\n","        super().__init__(*args, **kwargs)\n","        self.residual_beta = residual_beta\n","        self.rrdb = nn.Sequential(*[DenseResidualBlock(in_channels, beta=residual_beta) for _ in range(3)])\n","\n","    def forward(self, x):\n","        return self.rrdb(x) * self.residual_beta + x\n","    \n","\n","class ModifiedSRCNN(nn.Module):\n","    def __init__(self, in_channels: int, num_blocks: int, \n","                 n1: int, n2: int, f1: int, f2: int, f3: int,\n","                 *args, **kwargs) -> None:\n","        \"\"\" Initialize the SRCNN with Dense Residual network model with the required layers \n","         Below params are the hyperparameters for the SRCNN model without the \n","         Bassic block which has been added extra other than the resisual connections.\n","        in_channels (int): Input number of channels\n","        num_blocks (int): Number of RRDB blocks\n","        n1 (int): Number of filters in the first convolutional layer\n","        n2 (int): Number of filters in the second convolutional layer\n","        f1 (int): Kernel size of the first convolutional layer\n","        f2 (int): Kernel size of the second convolutional layer\n","        f3 (int): Kernel size of the third convolutional layer\n","        residual_beta (float): Residual connection weight\n","        \"\"\"\n","        super().__init__(*args, **kwargs)\n","        self.conv1 = ConvBlock(in_channels, n1, kernel_size=f1, stride=1, padding=(1, 1, 1), use_activation=True)\n","        self.bn1 = nn.BatchNorm3d(n1)\n","        self.blocks = nn.Sequential(*[RRDB(n1 + in_channels, residual_beta=0.5) for _ in range(num_blocks)])\n","        self.bn2 = nn.BatchNorm3d((n1 + in_channels))\n","        self.conv2 = ConvBlock(2 * (n1 + in_channels), n2, kernel_size=f2, stride=1, padding=(1, 1, 1), use_activation=True)\n","        self.bn3 = nn.BatchNorm3d(n2)\n","        self.conv3 = ConvBlock(n2 + n1 + in_channels, in_channels, kernel_size=f3, stride=1, padding=1, use_activation=False)\n","        self.bn4 = nn.BatchNorm3d(in_channels)\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        \n","        with torch.cuda.amp.autocast():\n","            initial = x \n","            x = self.conv1(x)\n","            x = self.bn1(x)\n","            x = torch.concat([x, initial], dim = 1)\n","            initial = x\n","            x = self.blocks(x)\n","            x = self.bn2(x)\n","            x = torch.concat([x, initial], dim = 1)\n","            x = self.conv2(x) # Take feature maps here\n","            x = self.bn3(x)\n","            x = torch.concat([x, initial], dim=1)\n","            x = self.conv3(x)\n","            x = self.bn4(x)\n","        return x \n","    \n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                torch.nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    torch.nn.init.zeros_(m.bias)"]},{"cell_type":"markdown","metadata":{},"source":["## Training phase\n","In this block of code, we write the training loop of the model. The model creates a folder called `Logs` during its training. In this folder, (1) it creates a `logs.log` file where it saves the validation losses and the training losses per epoch along with the epoch time and (2) It saves the best weights of the model.\n","An `Early stopping Callback` has been implemented with a patience of `p` for effective training to prevent overfitting of the model."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T19:48:39.055082Z","iopub.status.busy":"2024-08-02T19:48:39.054631Z","iopub.status.idle":"2024-08-02T19:48:39.072516Z","shell.execute_reply":"2024-08-02T19:48:39.071384Z","shell.execute_reply.started":"2024-08-02T19:48:39.055043Z"},"trusted":true},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, \n","                num_epochs, device, log_folder, patience=5):\n","    \"\"\" Train the model using the specified data loaders and hyperparameters.\n","    Saves the best model weights based on the validation loss.\n","    Args:\n","        model (torch.nn.Module): Model to be trained\n","        train_loader (torch.utils.data.DataLoader): Training data loader\n","        val_loader (torch.utils.data.DataLoader): Validation data loader\n","        criterion (torch.nn.Module): Loss function\n","        optimizer (torch.optim.Optimizer): Optimizer\n","        num_epochs (int): Number of epochs to train the model\n","        device (torch.device): Device to run the model on\n","        log_folder (str): Folder to store logs and model weights\n","        patience (int): Number of epochs to wait before early stopping\n","    Returns:\n","        torch.nn.Module: Trained model\n","    \"\"\"\n","    # Move model to the specified device\n","    model.to(device)\n","    \n","    # Create directories for storing artifacts\n","    os.makedirs(log_folder, exist_ok=True)\n","    \n","    log_file = os.path.join(log_folder, 'logs.log')\n","    best_weights_file = os.path.join(log_folder, 'best_weights.pth')\n","    \n","    best_loss = float('inf')\n","    patience_counter = 0\n","    \n","    with open(log_file, 'w') as log:\n","        log.write('Epoch,Train Loss,Val Loss,Epoch Time\\n')\n","        \n","        for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n","            start_time = time.time()\n","            \n","            # Training phase\n","            model.train()\n","            train_losses = []\n","            for hr_images, lr_images in train_loader:\n","                hr_images, lr_images = hr_images.to(device), lr_images.to(device)\n","                optimizer.zero_grad()\n","                \n","                # Enable autocast context for mixed precision training\n","                with autocast():\n","                    sr_images = model(lr_images)\n","                    loss = criterion(sr_images, hr_images)\n","                \n","                # Backward pass\n","                loss.backward()\n","                \n","                # Clip gradients to prevent exploding gradients\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                \n","                optimizer.step()\n","                \n","                train_losses.append(loss.item())\n","            \n","            train_loss = np.mean(train_losses)\n","            \n","            # Validation phase\n","            model.eval()\n","            val_losses = []\n","            with torch.no_grad():\n","                for hr_images, lr_images in val_loader:\n","                    hr_images, lr_images = hr_images.to(device), lr_images.to(device)\n","                    with autocast():\n","                        sr_images = model(lr_images)\n","                        loss = criterion(sr_images, hr_images)\n","                    val_losses.append(loss.item())\n","            \n","            val_loss = np.mean(val_losses)\n","            \n","            epoch_time = time.time() - start_time\n","            \n","            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Epoch Time: {epoch_time:.2f}s')\n","            log.write(f'{epoch+1},{train_loss},{val_loss},{epoch_time}\\n')\n","            \n","            # Check for best validation loss\n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                patience_counter = 0\n","                torch.save(model.state_dict(), best_weights_file)\n","            else:\n","                patience_counter += 1\n","                        \n","            if patience_counter >= patience:\n","                print(f'Early stopping at epoch {epoch+1}')\n","                break\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Configurations \n","Here, we define the model hyperparameters for easy and compact access throughout the training process."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T19:48:39.074140Z","iopub.status.busy":"2024-08-02T19:48:39.073776Z","iopub.status.idle":"2024-08-02T19:48:39.173654Z","shell.execute_reply":"2024-08-02T19:48:39.167785Z","shell.execute_reply.started":"2024-08-02T19:48:39.074113Z"},"trusted":true},"outputs":[],"source":["NUM_EPOCHS: int = 1\n","PATIENCE: int = 15\n","BATCH_SIZE: int= 8\n","LOG_FOLDER: str = \"Logs\"\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","######### Dataloader hyperparameters ########\n","LATITUDE_CHUNK_SIZE = 16\n","LONGITUDE_CHUNK_SIZE = 16\n","\n","######### MODEL HYPERPARAMETERS #########\n","in_channels = 7\n","num_blocks = 2\n","n1 = 32\n","n2 = 128\n","f1 = 3\n","f2 = 3\n","f3 = 3\n","LEARNING_RATE: int = 3e-4"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T19:48:39.187770Z","iopub.status.busy":"2024-08-02T19:48:39.182963Z","iopub.status.idle":"2024-08-02T19:48:58.508960Z","shell.execute_reply":"2024-08-02T19:48:58.508099Z","shell.execute_reply.started":"2024-08-02T19:48:39.187710Z"},"trusted":true},"outputs":[],"source":["ozone_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_2011.nc\")\n","co_no2_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_CO_NO2.nc\")\n","no_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_NO.nc\")\n","humidity_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_Archi_2011_SpecificHum.nc\")\n","temp_2011 = xr.open_dataset(\"/Volumes/Extreme SSD/PRL/data/high_res/WRF_2011_Archi_T.nc\")\n","\n","\n","PRESSURE_LEVEL = 2\n","high_res_data = np.array([ozone_2011[\"o3\"].sel(bottom_top=PRESSURE_LEVEL), \n","            ozone_2011[\"PM2_5_DRY\"].sel(bottom_top=PRESSURE_LEVEL),\n","            co_no2_2011[\"co\"].sel(bottom_top=PRESSURE_LEVEL), \n","            co_no2_2011[\"no2\"].sel(bottom_top=PRESSURE_LEVEL), \n","            no_2011[\"no\"].sel(bottom_top=PRESSURE_LEVEL), \n","            humidity_2011[\"QVAPOR\"].sel(bottom_top=PRESSURE_LEVEL),\n","            temp_2011[\"T2\"]])\n","\n","low_res_data = high_res_data + np.random.rand(*high_res_data.shape)\n","min_max_transform = MinMaxScaleTransform(high_res_data, low_res_data, use_half=True)\n","\n","dataset = WRFDataset(high_res_data, low_res_data, \n","                     LATITUDE_CHUNK_SIZE, \n","                     LONGITUDE_CHUNK_SIZE,   \n","                     transform=min_max_transform)\n","\n","train_loader, valid_loader, test_loader = create_loaders(dataset, BATCH_SIZE)\n","train_loader, valid_loader, test_loader = train_loader, valid_loader, test_loader"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T19:48:58.510359Z","iopub.status.busy":"2024-08-02T19:48:58.510039Z","iopub.status.idle":"2024-08-02T19:49:00.312846Z","shell.execute_reply":"2024-08-02T19:49:00.311731Z","shell.execute_reply.started":"2024-08-02T19:48:58.510330Z"},"trusted":true},"outputs":[],"source":["# TRAINING PART\n","model_srcnn = ModifiedSRCNN(in_channels=in_channels, num_blocks=num_blocks, n1=n1, n2=n2, f1=f1, f2=f2, f3=f3)\n","model_srcnn = model_srcnn.half().to(DEVICE)\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model_srcnn.parameters(), lr=LEARNING_RATE)\n","# train_model(model = model_srcnn, train_loader=train_loader, val_loader=valid_loader, \n","#             criterion=criterion, optimizer=optimizer, num_epochs=NUM_EPOCHS, \n","#             log_folder=LOG_FOLDER, device=DEVICE, patience=PATIENCE)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-02T19:49:29.951826Z","iopub.status.busy":"2024-08-02T19:49:29.951463Z","iopub.status.idle":"2024-08-02T19:49:30.443329Z","shell.execute_reply":"2024-08-02T19:49:30.441902Z","shell.execute_reply.started":"2024-08-02T19:49:29.951798Z"},"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacty of 14.74 GiB of which 2.12 MiB is free. Process 6862 has 14.74 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 409.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m hr, lr \u001b[38;5;241m=\u001b[39m hr\u001b[38;5;241m.\u001b[39mto(DEVICE), lr\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m----> 8\u001b[0m         sr \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_srcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(sr, hr)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[3], line 72\u001b[0m, in \u001b[0;36mModifiedSRCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m     71\u001b[0m     initial \u001b[38;5;241m=\u001b[39m x \n\u001b[0;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([x, initial], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    604\u001b[0m     )\n\u001b[0;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacty of 14.74 GiB of which 2.12 MiB is free. Process 6862 has 14.74 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 409.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["torch.cuda.empty_cache()\n","torch.cuda.reset_peak_memory_stats()\n","model_srcnn.eval()\n","for idx, (hr, lr) in enumerate(train_loader):\n","    hr, lr = hr.to(DEVICE), lr.to(DEVICE)\n","    \n","    with autocast():\n","            sr = model_srcnn(lr)\n","            loss = criterion(sr, hr)\n","    print(f\"Epoch = {idx}, loss = {loss}\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5485921,"sourceId":9091067,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
